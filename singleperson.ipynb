{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本的引入\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torchvision import transforms as T\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改编版！新增monitor(mIoU)\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, monitor=\"val_acc\", patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            monitor (string): 可以选 \"val_acc\"or \"val_loss\"or\"val_mIoU\"\n",
    "                            \n",
    "                            Default: \"val_acc\"\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.monitor=monitor\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.val_acc_max = 0\n",
    "        self.val_mIoU_max = 0\n",
    "        self.delta = delta\n",
    "    \n",
    "    def __call__(self, val, model):\n",
    "        if self.monitor=='val_loss':\n",
    "            val_loss=val\n",
    "            score = -val_loss\n",
    "\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model)\n",
    "                self.counter = 0\n",
    "        elif self.monitor=='val_acc':\n",
    "            #这里的val是0-100之间的数。\n",
    "            val_acc=val\n",
    "            score = val_acc\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_acc, model)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_acc, model)\n",
    "                self.counter = 0   \n",
    "        elif self.monitor=='val_mIoU':\n",
    "            #这里的val是0-100之间的数。\n",
    "            val_mIoU=val\n",
    "            score = val_mIoU\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_mIoU, model)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_mIoU, model)\n",
    "                self.counter = 0 \n",
    "    def save_checkpoint(self, val, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        '''Saves model when validation accuracy increase.'''\n",
    "        if self.monitor=='val_loss':\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val:.6f}).  Saving model ...')           \n",
    "            self.val_loss_min = val\n",
    "        if self.monitor=='val_acc':\n",
    "            if self.verbose:\n",
    "                print(f'Validation accuracy increased ({self.val_acc_max:.6f}% --> {val:.6f}%).  Saving model ...')\n",
    "            self.val_acc_max = val\n",
    "        if self.monitor=='val_mIoU':\n",
    "            if self.verbose:\n",
    "                print(f'Validation mIoU increased ({self.val_mIoU_max:.6f} --> {val:.6f}).  Saving model ...')\n",
    "            self.val_mIoU_max = val\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\t# 这里会存储迄今最优模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了可以同时处理image和mask，设置了一下的函数\n",
    "def my_transform1(image, mask):\n",
    "    #my_transform1是针对训练集的，可以做到data augmentation的效果\n",
    "    if random.random() > 0.5:\n",
    "        image = tf.hflip(image)\n",
    "        mask = tf.hflip(mask)\n",
    "    if random.random() > 0.5:\n",
    "        image = tf.vflip(image)\n",
    "        mask = tf.vflip(mask)\n",
    "    #对image进行resize，totensor，还有normalize\n",
    "    transform_image = T.Compose([   \n",
    "        T.Resize([256,256]),        \n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    image = transform_image(image)\n",
    "    #对mask进行resize，把它转换成array，把所有大于1的地方换成1，最后转成tensor。\n",
    "    mask=T.functional.resize(mask,(256,256))\n",
    "    mask=np.array(mask)\n",
    "    mask=(mask>=1).astype(int)\n",
    "    mask = T.functional.to_tensor(mask).float()\n",
    "    return image, mask\n",
    "    \n",
    "def my_transform2(image, mask):\n",
    "    #my_transform2是针对valid&test的，所以就不需要rotation之类的处理了。\n",
    "    #对image进行resize，totensor，还有normalize\n",
    "    transform_image = T.Compose([\n",
    "        T.Resize([256, 256]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean= [0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    image = transform_image(image)\n",
    "    #对mask进行resize，把它转换成array，把所有大于1的地方换成1，最后转成tensor。\n",
    "    mask=T.functional.resize(mask,(256,256))\n",
    "    mask=np.array(mask)\n",
    "    mask=(mask>=1).astype(int)\n",
    "    mask = T.functional.to_tensor(mask).float()\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path=None, mask_path=None, transform=None,data=\"train\"):   \n",
    "        \"\"\"\n",
    "        初始化自定义Dataset类的参数\n",
    "        Attributes\n",
    "            file_path: 字符串，数据集的存储路径，例如‘./UCF101/train’ 或 './UCF101/eval'等\n",
    "            mask_path: 字符串，数据集的存储路径，例如‘./UCF101/train_annotation’等\n",
    "            transform: 传入一个从torchvision.transforms定义的数据预处理\n",
    "        \"\"\"\n",
    "        self.count=0\n",
    "        self.transform = transform\n",
    "        if data=='train':\n",
    "            self.name=open(os.path.join(\"../input/singleperson/TrainVal_images\",\"train_id.txt\"))\n",
    "        elif data==\"valid\":\n",
    "            self.name=open(os.path.join(\"../input/singleperson/TrainVal_images\",\"val_id.txt\"))\n",
    "            #我要改变一下，就是这里的val_id会被分出去一些,就是10000的一半！\n",
    "        elif data==\"test\":\n",
    "            self.name=open(os.path.join(\"../input/singleperson/TrainVal_images\",\"val_id.txt\"))\n",
    "        # 初始化给定文件夹下的所有数据\n",
    "        self.init_all_data(file_path,mask_path,data) \n",
    "\n",
    "        return None\n",
    "        \n",
    "\n",
    "    def init_all_data(self,file_path,mask_path,data):\n",
    "        \"\"\"\n",
    "        初始化该数据集内所有的图像及其对应的标签，保存在self.images和self.labels两个列表内\n",
    "        Attributes\n",
    "            file_path: 字符串，数据集文件夹的存储路径\n",
    "            mask_path: 字符串，数据集文件夹的存储路径\n",
    "        \"\"\"\n",
    "        # 初始化两个列表，记录该数据集内每一张图片的完整路径及其对应的mask\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for name in self.name:\n",
    "            # 得当当前图片的完整路径，若是有效图片，则记录该图片\n",
    "            self.count+=1\n",
    "            if data==\"train\"and self.count<=2000:\n",
    "                img = os.path.join(file_path,name[0:-1]+\".jpg\")\n",
    "                mask = os.path.join(mask_path,name[0:-1]+\".png\")\n",
    "                if self.is_valid_image(img) and self.is_valid_image(mask):\n",
    "                    self.images.append(img)\n",
    "                    self.labels.append(mask)\n",
    "            if data==\"valid\" and self.count<=600:\n",
    "                img = os.path.join(file_path,name[0:-1]+\".jpg\")\n",
    "                mask = os.path.join(mask_path,name[0:-1]+\".png\")\n",
    "                if self.is_valid_image(img) and self.is_valid_image(mask):\n",
    "                    self.images.append(img)\n",
    "                    self.labels.append(mask)\n",
    "            elif data==\"test\" and self.count>=5000:\n",
    "                img = os.path.join(file_path,name[0:-1]+\".jpg\")\n",
    "                mask = os.path.join(mask_path,name[0:-1]+\".png\")\n",
    "                if self.is_valid_image(img) and self.is_valid_image(mask):\n",
    "                    self.images.append(img)\n",
    "                    self.labels.append(mask)\n",
    "        return None\n",
    "\n",
    "        \n",
    "    def is_valid_image(self, img_path):\n",
    "        \"\"\"\n",
    "        判断图片是否为可以打开的有效文件\n",
    "        Attributes\n",
    "            img_path: 字符串，待检测图片的存储路径\n",
    "        Returns\n",
    "            valid: 布尔变量，True/False分别表示该图片是否可以正常打开\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 若读取成功，设valid为True\n",
    "            i = Image.open(img_path)\n",
    "            valid = True\n",
    "        except:\n",
    "            # 若读取失败，设valid为False\n",
    "            valid = False\n",
    "            \n",
    "        return valid\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        按给定索引，获取对应的图片及其标签\n",
    "        Attributes\n",
    "            idx: int类型数字，表示目标图像的索引\n",
    "        Returns\n",
    "            image: 一个打开的PIL.Image对象，是PIL库存储图像的一种数据格式（类似于OpenCV利用numpy张量存储图像）\n",
    "            label: Image类型，表示对应的mask\n",
    "        \"\"\"\n",
    "        # 利用PIL.Image.open打开图片，并将其强制转化为RGB格式（防止数据集中混杂灰度图，导致读取出单通道图片，送入网络因矩阵维度不一致而报错）\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        # 获取对应的mask\n",
    "        label = Image.open(self.labels[idx])\n",
    "        #获取mask后就要把它转换成全0，1的array，再换成Image\n",
    "        \n",
    "        # 进行预处理的变换\n",
    "        if self.transform:\n",
    "            image,label = self.transform(image,label)\n",
    "        return image, label\n",
    "   \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        获取数据集中图像的总数，该方法的作用是用于DataLoader去调用，从而获取在给定Batch Size的情况下，一个Epoch的总长，\n",
    "        从而可以在一个Epoch结束时实现shuffle数据集的功能\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(\"../input/singleperson/TrainVal_images/TrainVal_images/train_images\",\n",
    "                       \"../input/singleperson/TrainVal_parsing_annotations/TrainVal_parsing_annotations/TrainVal_parsing_annotations/train_segmentations\",\n",
    "                       transform=my_transform1,\n",
    "                      data=\"train\")\n",
    "valid_data=MyDataset(\"../input/singleperson/TrainVal_images/TrainVal_images/val_images\",\n",
    "                       \"../input/singleperson/TrainVal_parsing_annotations/TrainVal_parsing_annotations/TrainVal_parsing_annotations/val_segmentations\",\n",
    "                       transform=my_transform2,\n",
    "                      data=\"valid\")\n",
    "test_data=MyDataset(\"../input/singleperson/TrainVal_images/TrainVal_images/val_images\",\n",
    "                       \"../input/singleperson/TrainVal_parsing_annotations/TrainVal_parsing_annotations/TrainVal_parsing_annotations/val_segmentations\",\n",
    "                       transform=my_transform2,\n",
    "                      data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_workers=2\n",
    "train_loader=data.DataLoader(dataset=train_data,batch_size=32,\n",
    "                             shuffle=True, num_workers=Num_workers)\n",
    "valid_loader=data.DataLoader(dataset=valid_data,batch_size=32,\n",
    "                             shuffle=True, num_workers=Num_workers)\n",
    "test_loader=Data.DataLoader(dataset=test_data,batch_size=32,\n",
    "                             shuffle=True, num_workers=Num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算IoU的function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(inputs,targets,smooth=1):\n",
    "    #把inputs，targets转成cpu再detach，这样就不会占用GPU资源。\n",
    "    inputs = inputs.cpu().detach().numpy()\n",
    "    targets = targets.cpu().detach().view(-1)\n",
    "    #要对input进行threshold，让他变成（0，1）组成的。\n",
    "    inputs=torch.tensor((inputs>0.5).astype(np.int32)).view(-1)\n",
    "    #intersection is equivalent to True Positive count\n",
    "    #union is the mutually inclusive area of all labels & predictions \n",
    "    intersection = (inputs * targets).sum()\n",
    "    total = (inputs + targets).sum()\n",
    "    union = total - intersection \n",
    "    IoU = (intersection + smooth)/(union + smooth)\n",
    "    return IoU.numpy()\n",
    "#return是numpy这样占的内存就不会过大了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#新增mIoU(其实用的是IoU)如果要改成mIoU只要把里面的两处给改了。\n",
    "def train_model(model,device, patience, n_epochs):\n",
    "    \n",
    "    # to track the training loss as the model trains\n",
    "    train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "    valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "    avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "    avg_valid_losses = [] \n",
    "    # to track the training mIoU as the model trains\n",
    "    train_mIoU = []\n",
    "    # to track the valid mIoU as the model trains\n",
    "    valid_mIoU = []\n",
    "    # to track the average training mIoU per epoch as the model trains \n",
    "    avg_train_mIoU = []\n",
    "    # to track the average validation mIoU per epoch as the model trains\n",
    "    avg_valid_mIoU = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "    early_stopping = EarlyStopping(\"val_mIoU\",patience=patience, verbose=True,delta=0)\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    " \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        \n",
    "        for step, (X, y) in enumerate(train_loader):\n",
    "            \n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(X)\n",
    "            # calculate the loss\n",
    "            loss = loss_func(output, y)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss and training mIoU\n",
    "            train_losses.append(loss.item())\n",
    "            train_mIoU.append(IoU(output, y)) \n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "     \n",
    "        for step, (X, y) in enumerate(valid_loader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(X)\n",
    "            # calculate the loss\n",
    "            loss = loss_func(output, y)\n",
    "            # record validation loss and valid mIoU\n",
    "            valid_losses.append(loss.item())\n",
    "            valid_mIoU.append(IoU(output, y)) \n",
    "            \n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        train_mIoU= np.average( train_mIoU)\n",
    "        valid_mIoU= np.average( valid_mIoU)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "        avg_train_mIoU.append(train_mIoU)\n",
    "        avg_valid_mIoU.append(valid_mIoU)\n",
    "        \n",
    "        epoch_len = len(str(n_epochs))\n",
    "        \n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'train_mIoU: {train_mIoU:.5f} ' +\n",
    "                     f'\\n    valid_loss: {valid_loss:.5f} ' +\n",
    "                     f'valid_mIoU: {valid_mIoU:.5f} ' )\n",
    "        \n",
    "        print(print_msg)\n",
    "        # early_stopping needs the validation acc to check if it has incresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        early_stopping(valid_mIoU, model)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "        train_mIoU = []\n",
    "        valid_mIoU = []\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    " \n",
    "    return  model, avg_train_losses, avg_valid_losses,avg_train_mIoU,avg_valid_mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是一个pytorch提供的大脑扫描pretrain的U-net\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=False).to(device)\n",
    "#网址：https://pytorch.org/hub/mateuszbuda_brain-segmentation-pytorch_unet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于loss function，我试了以下几个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = torch.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = torch.sigmoid(inputs)    \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        #intersection is equivalent to True Positive count\n",
    "        #union is the mutually inclusive area of all labels & predictions \n",
    "        intersection = (inputs * targets).sum()\n",
    "        total = (inputs + targets).sum()\n",
    "        union = total - intersection \n",
    "        \n",
    "        IoU = (intersection + smooth)/(union + smooth)\n",
    "                \n",
    "        return 1 - IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        #inputs = torch.sigmoid(inputs)    \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
    "        Dice_BCE = BCE + dice_loss\n",
    "        \n",
    "        return Dice_BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr,weight_decay=1e-5)\n",
    "#loss_func = torch.nn.MSELoss()  \n",
    "#loss_func = torch.nn.BCELoss()  \n",
    "#loss_func = DiceLoss()\n",
    "loss_func = IoULoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=100\n",
    "patience = 7\n",
    "#optimizer=torch.optim.Adam(model.parameters(),lr=lr,weight_decay=1e-4)\n",
    "model, train_loss, valid_loss,train_mIoU,valid_mIoU = train_model(model ,device, patience, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化单个图片的训练结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印一下图片，ground truth 和我做出的mask，以及threshold后的mask\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "plt.figure(dpi = 600)#让图片清晰些\n",
    "#导入要验证的图片\n",
    "image,label=valid_data.__getitem__(190)\n",
    "#打印原图\n",
    "plt.subplot(1,4,1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(np.transpose(image.numpy(),(1,2,0)))\n",
    "#打印ground truth\n",
    "plt.subplot(1,4,2)\n",
    "plt.title(\"ground truth\")\n",
    "plt.imshow(label.numpy()[0],cmap=\"gray\")\n",
    "#打印我做出来的mask\n",
    "img = torch.unsqueeze(image,dim=0)\n",
    "b_x=img.cuda()\n",
    "out=model(b_x).to(torch.float64)\n",
    "out=out.cpu().detach().numpy()[0][0]\n",
    "plt.subplot(1,4,3)\n",
    "plt.title(\"output mask\")\n",
    "plt.imshow(out,cmap=\"gray\")\n",
    "#打印threshold后的mask\n",
    "plt.subplot(1,4,4)\n",
    "plt.title(\"threshold mask\")\n",
    "out=out>0.5\n",
    "plt.imshow(out,cmap=\"gray\")\n",
    "#保存图片\n",
    "plt.savefig('valid_190_BCE.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化loss和mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the loss as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "minposs = valid_loss.index(min(valid_loss))+1 \n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "#plt.ylim(0, 0.5) # consistent scale\n",
    "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('Dice_loss.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the mIoU as the network trained\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_mIoU)+1),train_mIoU, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_mIoU)+1),valid_mIoU,label='Validation Loss')\n",
    "\n",
    "# find position of lowest validation loss\n",
    "maxposs = valid_mIoU.index(max(valid_mIoU))+1 \n",
    "plt.axvline(maxposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "#plt.ylim(0, 0.5) # consistent scale\n",
    "plt.xlim(0, len(train_loss)+1) # consistent scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig('Dice_mIoU.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader,loss_func):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    test_mIoU=[]\n",
    "    for (X, y) in test_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(X)\n",
    "        # calculate the loss\n",
    "        loss = loss_func(output, y)\n",
    "        # record validation loss\n",
    "        test_loss.append(loss.item())\n",
    "        test_mIoU.append(IoU(output,y))   \n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Average mIoU: {:.4f}'.format(\n",
    "        np.average(test_loss),np.average(test_mIoU)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, device, test_loader,loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#打印一下图片，ground truth 和我做出的mask，以及threshold后的mask\n",
    "import matplotlib.pyplot as plt # plt 用于显示图片\n",
    "plt.figure(dpi = 600)#让图片清晰些\n",
    "#导入要验证的图片\n",
    "image,label=test_data.__getitem__(190)\n",
    "#打印原图\n",
    "plt.subplot(1,4,1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(np.transpose(image.numpy(),(1,2,0)))\n",
    "#打印ground truth\n",
    "plt.subplot(1,4,2)\n",
    "plt.title(\"ground truth\")\n",
    "plt.imshow(label.numpy()[0],cmap=\"gray\")\n",
    "#打印我做出来的mask\n",
    "img = torch.unsqueeze(image,dim=0)\n",
    "b_x=img.cuda()\n",
    "out=model(b_x).to(torch.float64)\n",
    "out=out.cpu().detach().numpy()[0][0]\n",
    "plt.subplot(1,4,3)\n",
    "plt.title(\"output mask\")\n",
    "plt.imshow(out,cmap=\"gray\")\n",
    "#打印threshold后的mask\n",
    "plt.subplot(1,4,4)\n",
    "plt.title(\"threshold mask\")\n",
    "out=out>0.5\n",
    "plt.imshow(out,cmap=\"gray\")\n",
    "#保存图片\n",
    "plt.savefig('test_190_BCE.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可视化多个图片的训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageshow(num_figure,model,dataloader):\n",
    "    #定义了一个打印多张图片的function\n",
    "    fig, axes = plt.subplots(num_figure, 4,dpi = 600, figsize=(7, 6))\n",
    "    imgs=np.arange(num_figure)*10\n",
    "    for i in imgs:\n",
    "        #导入要验证的图片\n",
    "        image,label=dataloader.__getitem__(i)\n",
    "        i=int(i/10)#设置index\n",
    "        #打印原图\n",
    "        axes[i][0].imshow(np.transpose(image.numpy(),(1,2,0)))\n",
    "        #打印ground truth\n",
    "        axes[i][1].imshow(label.numpy()[0],cmap='gray')\n",
    "        #打印我做出来的mask\n",
    "        img = torch.unsqueeze(image,dim=0)\n",
    "        b_x=img.cuda()\n",
    "        out=model(b_x).to(torch.float64)\n",
    "        out=out.cpu().detach().numpy()[0][0]\n",
    "        axes[i][2].imshow(out,cmap=\"gray\")\n",
    "        #打印threshold后的mask\n",
    "        out=out>0.5\n",
    "        axes[i][3].imshow(out,cmap=\"gray\")  \n",
    "    for ax in axes.ravel():\n",
    "        ax.axis('off')#关掉坐标轴       \n",
    "    fig.tight_layout() #让图片紧密 \n",
    "    #设置标签\n",
    "    axes[0][0].set_title(\"Original image\")\n",
    "    axes[0][1].set_title(\"Ground truth\")\n",
    "    axes[0][2].set_title(\"Output mask\")\n",
    "    axes[0][3].set_title(\"Threshold mask\")\n",
    "    #保存图片\n",
    "    fig.savefig('imshow.png')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageshow(6,model,valid_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
